---
layout: post
title:  "EdTech for peer review: my experience"
tags: education technology
excerpt: "What I learned from testing a novel peer assessment app"
toc: true
---
## The context

At the start of this academic year, my employer started working with an EdTech company who provides a series of products targeted at Universities.
At that time, we were still in the dip between the first and the second wave of the Covid-19 pandemic and we were dreaming that we would be going back to live on-campus courses by the end of autumn.

### The Web Development course

One of the courses that would start towards the end of the first semester had been redesigned just a year before.
It is a second year course in our bachelor's programme where students get an introduction to web development.
The learning curve is quite steep, since this is the most technical course in the curriculum, but this curve is addressed by giving exercises of increasing complexity to complete over a number of weeks.
In a pre-pandemic situation, this would happen in a classroom, and students would get multiple opportunities to try and fix their work while lecturers walk around the class.

Last year, the pandemic hit halfway through that course, and this way of teaching stopped working.
We know had to schedule 5-minute video slots for feedback, and we had no opportunity to "walk around the class" and come back to the same students before the next scheduled moment, one week later.
It was also much harder for students to help each other than when they were sitting next to each other.

We therefore knew that we couldn't teach the same way through a second lockdown.
We were also affected by the decision to let all first year students go through, and we had bigger classes than the year before (about 50 students for each of the three lecturers on the course).
The peer review platform from our EdTech partner seemed to offer a great solution:
for each of the web development exercises, our students would deliver their work on the platform, they would then review each other, and we would give a final layer of feedback to double-check.

We created 5 peer-review assignments for each of our classes.
Each of these assignments was graded on a list of "pass-fail" criteria, e.g. the absence of technical errors, or the use of specific techniques taught through recorded video tutorials.
Each student would be assigned randomly two peers from the same class.
If a student passes the peer assessment, they then get assessed by a lecturer.
If they fail, they get another chance.

### The Design Thinking course

I also decided to experiment with peer feedback on another course, this time in our master.
The course follows roughly the 5-step "Design Thinking" model created by the Hasso Plattner Institute of Design at Stanford.
At several points throughout the process, students deliver a draft and receive feedback on it.
In past years, only lecturers would provide feedback, but we decided to use the same peer review tool to gather feedback.

The experience that students and lecturers had with the peer review tool was radically different in both courses.

## The result

The implementation of peer review had much better results in the Design Thinking course.
After describing how it went in each course and drawing pros and cons from each, I reflect on the tedious aspects of setting up this technology.

### The good: the successful course

In the Design Thinking course, we received praise from students for our innovative use of technology.

On the lecturer side, we were also happy with the quality of comments that we saw: each group received valuable insight on the content, the writing and the structure of their document.
Comments revealed interesting questions about the course and we could continue the discussion when we met in class.

Students were happy to see what each other were doing, and this also helped with keeping them motivated and on track to hand in the final report.

### The bad: the less successful course

On the other hand, in Web Development, students were very critical of the same product, and feedback was also of a lower quality.

#### So, what was different?

The first major difference was that in the Design Thinking course, we focused on gathering feedback, while in Web Development

### The ugly: the burden of self-service software

New technologies don't always simplify our jobs, but often generate more work for ourselves, and this tool was no exception.

We had to:
- Set up each assignment, which includes:
  - Writing instructions
  - Writing grading criteria
  - Setting up deadlines
- Distribute the assignment through our Information Systems
- Collect assignments and reviews
- Make our own review of the assignments
- Keep track of the results (for the web development course, this meant maintaining a big spreadsheets with a dozen tabs and many formulas)
- Communicating the results to students (which can be automated using mailings, but this also requires technical competencies)

Not all these steps are necessary depending on how peer review is included in the course, but in our case, this was an additional burden.
We did receive some guidance at the very start, but we had to learn and try out several different options.

The platform we used did provide options for data export

## Improving peer review

The first series of lessons from this experience is how to improve peer reviewing in general, whether or not we use a technological platform for it.

### Framing is important

### Reviewing is a skill

### Peer-reviewing exposes our own flaws

### Feedback is a continuous process

## Improving EdTech products

### Context is key

### The self-fulfilling promises of new products

### Keep it simple?

## Conclusion
